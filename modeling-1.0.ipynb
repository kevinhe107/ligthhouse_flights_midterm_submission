{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "In this file, instructions how to approach the challenge can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work on different types of Machine Learning problems:\n",
    "\n",
    "- **Regression Problem**: The goal is to predict delay of flights.\n",
    "- **(Stretch) Multiclass Classification**: If the plane was delayed, we will predict what type of delay it is (will be).\n",
    "- **(Stretch) Binary Classification**: The goal is to predict if the flight will be cancelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Task: Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is **ARR_DELAY**. We need to be careful which columns to use and which don't. For example, DEP_DELAY is going to be the perfect predictor, but we can't use it because in real-life scenario, we want to predict the delay before the flight takes of --> We can use average delay from earlier days but not the one from the actual flight we predict.  \n",
    "\n",
    "For example, variables **CARRIER_DELAY, WEATHER_DELAY, NAS_DELAY, SECURITY_DELAY, LATE_AIRCRAFT_DELAY** shouldn't be used directly as predictors as well. However, we can create various transformations from earlier values.\n",
    "\n",
    "We will be evaluating your models by predicting the ARR_DELAY for all flights **1 week in advance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering will play a crucial role in this problems. We have only very little attributes so we need to create some features that will have some predictive power.\n",
    "\n",
    "- weather: we can use some weather API to look for the weather in time of the scheduled departure and scheduled arrival.\n",
    "- statistics (avg, mean, median, std, min, max...): we can take a look at previous delays and compute descriptive statistics\n",
    "- airports encoding: we need to think about what to do with the airports and other categorical variables\n",
    "- time of the day: the delay probably depends on the airport traffic which varies during the day.\n",
    "- airport traffic\n",
    "- unsupervised learning as feature engineering?\n",
    "- **what are the additional options?**: Think about what we could do more to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = pd.read_csv('flights_rand.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1 = flights.drop(['dep_time', 'actual_elapsed_time', 'air_time','dep_delay', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'origin', 'branded_code_share', 'carrier_delay','weather_delay','nas_delay','security_delay', 'late_aircraft_delay', 'first_dep_time', 'total_add_gtime','longest_add_gtime','cancelled', 'cancellation_code', 'diverted', 'dup', 'flights', 'no_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_number_mode = flights1['tail_num'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1['tail_num'] = flights1['tail_num'].fillna(str(tail_number_mode[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1['arr_delay'] = flights1['arr_delay'].fillna(flights1['arr_delay'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* make the types categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1[\"mkt_carrier\"] = flights1[\"mkt_carrier\"].astype(\"category\")\n",
    "flights1[\"op_unique_carrier\"] = flights1[\"op_unique_carrier\"].astype(\"category\")\n",
    "flights1[\"tail_num\"] = flights1[\"tail_num\"].astype(\"category\")\n",
    "flights1[\"op_carrier_fl_num\"] = flights1[\"op_carrier_fl_num\"].astype(\"category\")\n",
    "flights1[\"origin_airport_id\"] = flights1[\"origin_airport_id\"].astype(\"category\")\n",
    "flights1[\"dest_airport_id\"] = flights1[\"dest_airport_id\"].astype(\"category\")\n",
    "flights1[\"mkt_carrier_fl_num\"] = flights1[\"mkt_carrier_fl_num\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* make arr_delay categorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1['arr_delay_cat'] = flights1['arr_delay'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE AIRPORTS AND TAILNUM\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "flights1['mkt_carrier'] = encoder.fit_transform(flights1[['mkt_carrier']])\n",
    "flights1['mkt_unique_carrier'] = encoder.fit_transform(flights1[['mkt_unique_carrier']])\n",
    "flights1['mkt_carrier_fl_num'] = encoder.fit_transform(flights1[['mkt_carrier_fl_num']])\n",
    "flights1['op_unique_carrier'] = encoder.fit_transform(flights1[['op_unique_carrier']])\n",
    "flights1['tail_num'] = encoder.fit_transform(flights1[['tail_num']])\n",
    "flights1['op_carrier_fl_num'] = encoder.fit_transform(flights1[['op_carrier_fl_num']])\n",
    "flights1['origin_airport_id'] = encoder.fit_transform(flights1[['origin_airport_id']])\n",
    "flights1['dest_airport_id'] = encoder.fit_transform(flights1[['dest_airport_id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adding month, day of the week, day of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1['fl_date'] = pd.to_datetime(flights1['fl_date'], errors='coerce')\n",
    "flights1['month'] = flights1['fl_date'].dt.month\n",
    "flights1['day_of_week'] = flights1['fl_date'].dt.dayofweek\n",
    "flights1['day_of_month'] = flights1['fl_date'].dt.day\n",
    "flights1['year'] = flights1['fl_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1[['origin_city_name_only', 'origin_city_name_short']] = flights1['origin_city_name'].str.split(',', expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1[['dest_city_name_only', 'dest_city_name_short']] = flights1['dest_city_name'].str.split(',', expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1 = flights1.drop(['origin_city_name_short', 'dest_city_name_short', 'origin_city_name', 'dest_city_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fl_date</th>\n",
       "      <th>mkt_unique_carrier</th>\n",
       "      <th>mkt_carrier</th>\n",
       "      <th>mkt_carrier_fl_num</th>\n",
       "      <th>op_unique_carrier</th>\n",
       "      <th>tail_num</th>\n",
       "      <th>op_carrier_fl_num</th>\n",
       "      <th>origin_airport_id</th>\n",
       "      <th>dest_airport_id</th>\n",
       "      <th>dest</th>\n",
       "      <th>...</th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>crs_elapsed_time</th>\n",
       "      <th>distance</th>\n",
       "      <th>arr_delay_cat</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>year</th>\n",
       "      <th>origin_city_name_only</th>\n",
       "      <th>dest_city_name_only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-07-27</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4431.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2219.0</td>\n",
       "      <td>4431.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>IDA</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>2018</td>\n",
       "      <td>Salt Lake City</td>\n",
       "      <td>Idaho Falls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-08</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4663.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3747.0</td>\n",
       "      <td>4663.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>ELP</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>667.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>Houston</td>\n",
       "      <td>El Paso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>718.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3048.0</td>\n",
       "      <td>718.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>DTW</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>2018</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Detroit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>CLT</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>1916.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2019</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Charlotte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-06-19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2624.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>431.0</td>\n",
       "      <td>2624.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>AUS</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>2018</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>Austin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4729.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3753.0</td>\n",
       "      <td>4729.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>MAF</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Midland/Odessa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>2018-11-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2179.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1468.0</td>\n",
       "      <td>2179.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>LAX</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>Santa Rosa</td>\n",
       "      <td>Los Angeles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2019-05-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1805.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>1805.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>IND</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>1488.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2019</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Indianapolis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2019-08-04</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2541.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>2541.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>BNA</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>562.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Nashville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>2018-05-27</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3378.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>ORD</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>2018</td>\n",
       "      <td>Kansas City</td>\n",
       "      <td>Chicago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fl_date  mkt_unique_carrier  mkt_carrier  mkt_carrier_fl_num  \\\n",
       "0    2018-07-27                 3.0          3.0              4431.0   \n",
       "1    2018-10-08                 8.0          8.0              4663.0   \n",
       "2    2018-10-31                10.0         10.0               718.0   \n",
       "3    2019-10-15                 0.0          0.0              1589.0   \n",
       "4    2018-06-19                 1.0          1.0              2624.0   \n",
       "...         ...                 ...          ...                 ...   \n",
       "9995 2019-01-12                 8.0          8.0              4729.0   \n",
       "9996 2018-11-02                 1.0          1.0              2179.0   \n",
       "9997 2019-05-20                 0.0          0.0              1805.0   \n",
       "9998 2019-08-04                10.0         10.0              2541.0   \n",
       "9999 2018-05-27                 8.0          8.0              2003.0   \n",
       "\n",
       "      op_unique_carrier  tail_num  op_carrier_fl_num  origin_airport_id  \\\n",
       "0                   7.0    2219.0             4431.0              289.0   \n",
       "1                  25.0    3747.0             4663.0              148.0   \n",
       "2                  24.0    3048.0              718.0              192.0   \n",
       "3                   2.0    1993.0             1589.0              168.0   \n",
       "4                  19.0     431.0             2624.0              286.0   \n",
       "...                 ...       ...                ...                ...   \n",
       "9995               25.0    3753.0             4729.0              148.0   \n",
       "9996               21.0    1468.0             2179.0              297.0   \n",
       "9997                2.0    3285.0             1805.0              237.0   \n",
       "9998               24.0     524.0             2541.0               83.0   \n",
       "9999               22.0    3378.0             2003.0              189.0   \n",
       "\n",
       "      dest_airport_id dest  ...  arr_delay  crs_elapsed_time  distance  \\\n",
       "0               131.0  IDA  ...       -2.0              64.0     188.0   \n",
       "1                83.0  ELP  ...       20.0             116.0     667.0   \n",
       "2                78.0  DTW  ...      -19.0              70.0     228.0   \n",
       "3                56.0  CLT  ...       -2.0             258.0    1916.0   \n",
       "4                17.0  AUS  ...      -16.0             216.0    1476.0   \n",
       "...               ...  ...  ...        ...               ...       ...   \n",
       "9995            172.0  MAF  ...      -11.0              94.0     429.0   \n",
       "9996            152.0  LAX  ...       -4.0              92.0     400.0   \n",
       "9997            133.0  IND  ...       -9.0             212.0    1488.0   \n",
       "9998             31.0  BNA  ...       33.0             110.0     562.0   \n",
       "9999            208.0  ORD  ...       -9.0              86.0     403.0   \n",
       "\n",
       "      arr_delay_cat  month  day_of_week  day_of_month  year  \\\n",
       "0                 0      7            4            27  2018   \n",
       "1                 1     10            0             8  2018   \n",
       "2                 0     10            2            31  2018   \n",
       "3                 0     10            1            15  2019   \n",
       "4                 0      6            1            19  2018   \n",
       "...             ...    ...          ...           ...   ...   \n",
       "9995              0      1            5            12  2019   \n",
       "9996              0     11            4             2  2018   \n",
       "9997              0      5            0            20  2019   \n",
       "9998              1      8            6             4  2019   \n",
       "9999              0      5            6            27  2018   \n",
       "\n",
       "      origin_city_name_only  dest_city_name_only  \n",
       "0            Salt Lake City          Idaho Falls  \n",
       "1                   Houston              El Paso  \n",
       "2                   Chicago              Detroit  \n",
       "3                 Las Vegas            Charlotte  \n",
       "4                  San Jose               Austin  \n",
       "...                     ...                  ...  \n",
       "9995                Houston       Midland/Odessa  \n",
       "9996             Santa Rosa          Los Angeles  \n",
       "9997                Phoenix         Indianapolis  \n",
       "9998             Washington            Nashville  \n",
       "9999            Kansas City              Chicago  \n",
       "\n",
       "[10000 rows x 22 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights1 = flights1.drop(['dest', 'dest_city_name_short', 'origin_city_name', 'dest_city_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1['origin_city_name_only'] = flights1['origin_city_name_only'].str.strip().str.lower()\n",
    "flights1['dest_city_name_only'] = flights1['dest_city_name_only'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join on weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['City'] = weather['City'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['date'] = pd.to_datetime(weather['StartTime(UTC)']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['EndTime(UTC)'] = pd.to_datetime(weather['EndTime(UTC)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = weather.groupby(['City', 'date']).max()['EndTime(UTC)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_merged = weather.merge(temp, on = ['City', 'date', 'EndTime(UTC)'], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_merged = weather_merged.drop(['City', 'StartTime(UTC)', 'EndTime(UTC)'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_merged = weather_merged.rename(columns = {'date': 'fl_date', 'origin_city_name_only': 'origin_city_name_only'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_merged['fl_date'] = pd.to_datetime(weather_merged['fl_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather = flights1.merge(weather_merged, on = ['origin_city_name_only', 'fl_date'], how = 'left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7415"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_weather['Type'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dropping flights_weather row null values, origin_city_name_only, dest_city_name_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather = flights_weather.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather = flights_weather.drop(['fl_date', 'origin_city_name_only'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather = flights_weather.drop(['dest_city_name_only', 'origin_city_name_only'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mkt_unique_carrier', 'mkt_carrier', 'mkt_carrier_fl_num',\n",
       "       'op_unique_carrier', 'tail_num', 'op_carrier_fl_num',\n",
       "       'origin_airport_id', 'dest_airport_id', 'crs_dep_time', 'crs_arr_time',\n",
       "       'arr_delay', 'crs_elapsed_time', 'distance', 'arr_delay_cat', 'month',\n",
       "       'day_of_week', 'day_of_month', 'year', 'Type', 'Severity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather = flights_weather.drop(['dest'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Categorize and encode weather and severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather[\"Type\"] = flights_weather[\"Type\"].astype(\"category\")\n",
    "flights_weather[\"Severity\"] = flights_weather[\"Severity\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather['Type'] = encoder.fit_transform(flights_weather[['Type']])\n",
    "flights_weather['Severity'] = encoder.fit_transform(flights_weather[['Severity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mkt_unique_carrier</th>\n",
       "      <th>mkt_carrier</th>\n",
       "      <th>mkt_carrier_fl_num</th>\n",
       "      <th>op_unique_carrier</th>\n",
       "      <th>tail_num</th>\n",
       "      <th>op_carrier_fl_num</th>\n",
       "      <th>origin_airport_id</th>\n",
       "      <th>dest_airport_id</th>\n",
       "      <th>crs_dep_time</th>\n",
       "      <th>crs_arr_time</th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>crs_elapsed_time</th>\n",
       "      <th>distance</th>\n",
       "      <th>arr_delay_cat</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4663.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3747.0</td>\n",
       "      <td>4663.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>955</td>\n",
       "      <td>1051</td>\n",
       "      <td>20.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>667.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>718.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3048.0</td>\n",
       "      <td>718.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1640</td>\n",
       "      <td>1850</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2051.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1605</td>\n",
       "      <td>1804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>1452.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1289.0</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>2229</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>2092.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1289.0</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>2229</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>2092.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10039</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>800</td>\n",
       "      <td>1049</td>\n",
       "      <td>17.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10042</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2196.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>2215</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10043</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1149.0</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1247</td>\n",
       "      <td>1618</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>931.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10050</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3702.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3233.0</td>\n",
       "      <td>3702.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1145</td>\n",
       "      <td>1352</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>1142.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10054</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4729.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3753.0</td>\n",
       "      <td>4729.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1445</td>\n",
       "      <td>1619</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2644 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mkt_unique_carrier  mkt_carrier  mkt_carrier_fl_num  op_unique_carrier  \\\n",
       "1                     8.0          8.0              4663.0               25.0   \n",
       "2                    10.0         10.0               718.0               24.0   \n",
       "9                     8.0          8.0               477.0               22.0   \n",
       "14                    3.0          3.0              1126.0                8.0   \n",
       "15                    3.0          3.0              1126.0                8.0   \n",
       "...                   ...          ...                 ...                ...   \n",
       "10039                 7.0          7.0               495.0               17.0   \n",
       "10042                 7.0          7.0               308.0               17.0   \n",
       "10043                 3.0          3.0              1376.0                8.0   \n",
       "10050                 3.0          3.0              3702.0               19.0   \n",
       "10054                 8.0          8.0              4729.0               25.0   \n",
       "\n",
       "       tail_num  op_carrier_fl_num  origin_airport_id  dest_airport_id  \\\n",
       "1        3747.0             4663.0              148.0             83.0   \n",
       "2        3048.0              718.0              192.0             78.0   \n",
       "9        2051.0              477.0              146.0             72.0   \n",
       "14       1289.0             1126.0               18.0            276.0   \n",
       "15       1289.0             1126.0               18.0            276.0   \n",
       "...         ...                ...                ...              ...   \n",
       "10039    2306.0              495.0              168.0             72.0   \n",
       "10042    2196.0              308.0              266.0             44.0   \n",
       "10043    1149.0             1376.0              212.0             71.0   \n",
       "10050    3233.0             3702.0              212.0             32.0   \n",
       "10054    3753.0             4729.0              148.0            172.0   \n",
       "\n",
       "       crs_dep_time  crs_arr_time  arr_delay  crs_elapsed_time  distance  \\\n",
       "1               955          1051       20.0             116.0     667.0   \n",
       "2              1640          1850      -19.0              70.0     228.0   \n",
       "9              1605          1804        0.0             239.0    1452.0   \n",
       "14             2019          2229      -10.0             310.0    2092.0   \n",
       "15             2019          2229      -10.0             310.0    2092.0   \n",
       "...             ...           ...        ...               ...       ...   \n",
       "10039           800          1049       17.0             109.0     628.0   \n",
       "10042          1949          2215      -20.0             146.0     919.0   \n",
       "10043          1247          1618      -11.0             151.0     931.0   \n",
       "10050          1145          1352      -24.0             187.0    1142.0   \n",
       "10054          1445          1619      -11.0              94.0     429.0   \n",
       "\n",
       "       arr_delay_cat  month  day_of_week  day_of_month  year  Type  Severity  \n",
       "1                  1     10            0             8  2018   4.0       1.0  \n",
       "2                  0     10            2            31  2018   1.0       2.0  \n",
       "9                  0     11            2             7  2018   1.0       4.0  \n",
       "14                 0      4            0            23  2018   4.0       1.0  \n",
       "15                 0      4            0            23  2018   4.0       2.0  \n",
       "...              ...    ...          ...           ...   ...   ...       ...  \n",
       "10039              1      4            5            21  2018   1.0       4.0  \n",
       "10042              0      1            2            31  2018   0.0       4.0  \n",
       "10043              0      9            3            20  2018   4.0       1.0  \n",
       "10050              0      5            2            30  2018   4.0       1.0  \n",
       "10054              0      1            5            12  2019   4.0       1.0  \n",
       "\n",
       "[2644 rows x 20 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection / Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply different selection techniques to find out which one will be the best for our problems.\n",
    "\n",
    "- Original Features vs. PCA conponents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PCA testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.decomposition import PCA\\nimport matplotlib.pyplot as plt'"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different ML techniques to predict each problem.\n",
    "\n",
    "- linear\n",
    "- Naive Bayes\n",
    "- Random Forest Regressor\n",
    "- SVM classification\n",
    "- XGBoost regresspr\n",
    "- The ensemble of your own choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickle module to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* target and features FOR WEATHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(flights_weather.arr_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = np.array(flights_weather.arr_delay_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = flights_weather.drop(['arr_delay', 'arr_delay_cat'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train test split and making samples FOR WEATHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.70,test_size=0.30, random_state=101, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating y_train_cat and y_test_cat\n",
    "X_train, X_test, y_train_cat, y_test_cat = model_selection.train_test_split(X, y_cat, train_size=0.70,test_size=0.30, random_state=101, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = pd.DataFrame(X_train).sample(frac = 0.1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sample = pd.DataFrame(y_train).sample(frac = 0.1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainCat_sample = pd.DataFrame(y_train_cat).sample(frac = 0.1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sample = pd.DataFrame(X_test).sample(frac = 0.1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scaler = StandardScaler()\\nX = scaler.fit_transform(X)'"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)'''\n",
    "'''y = (y - y.mean()) / y.std()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scaling PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler())])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler())])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('linear_model', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_model', LinearRegression())])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_linear = linear_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score = linear_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00012162642177759864"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_linear_pickle', 'wb') as linear_file:\n",
    "    pickle.dump(linear_pipe, linear_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_linear_pickle', 'rb') as linear_file:\n",
    "    model_linear = pickle.load(linear_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naives Bayes, GaussianNB Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_Gauss_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_Gauss_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('NB_Gauss_model', GaussianNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('NB_Gauss_model', GaussianNB())])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_Gauss_pipe.fit(X_train, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_NBGauss = NB_Gauss_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5654911838790933"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score = NB_Gauss_pipe.score(X_test, y_test_cat)\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_Gaussian_pickle', 'wb') as Gaussian_file:\n",
    "    pickle.dump(NB_Gauss_pipe, Gaussian_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_Gaussian_pickle', 'rb') as Gaussian_file:\n",
    "    model_Gaussian = pickle.load(Gaussian_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_class = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters ={\n",
    "    'rand_forrestClass__n_estimators': [8000, 10000],\n",
    "    'rand_forrestClass__max_depth': [10],\n",
    "    'rand_forrestClass__bootstrap': [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forest_class_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('rand_forrestClass', RandomForestClassifier())])\n",
    "forest_class_grid = GridSearchCV(estimator=Forest_class_pipe, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with sample to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forrestClass_grid = GridSearchCV(estimator=random_forest_class, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_class_grid.fit(X_train_sample, y_trainCat_sample.ravel())\n",
    "y_pred_randFor_class = forest_class_grid.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_class_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjusted to best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_class_grid.fit(X_train, y_train_cat.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_randFor_class = forest_class_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_class_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5616250137111254"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score = forest_class_grid.score(X_test, y_test_cat)\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_Forrest_class_pickle', 'wb') as forest_class_file:\n",
    "    pickle.dump(forest_class_grid, forest_class_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_Forrest_class_pickle', 'rb') as forest_class_file:\n",
    "    model_forrestClass = pickle.load(forest_class_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regression_model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'rand_forrestReg__n_estimators': [1000],\n",
    "    'rand_forrestReg__max_depth': [2],\n",
    "    'rand_forrestReg__min_samples_split':[6], \n",
    "    'rand_forrestReg__bootstrap':[True],\n",
    "    'rand_forrestReg__criterion' :['mse']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forest_reg_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('rand_forrestReg', RandomForestRegressor())])\n",
    "forest_reg_grid = GridSearchCV(estimator=Forest_reg_pipe, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with sample to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''forest_reg_grid.fit(X_train_sample, y_train_sample.ravel())\n",
    "y_pred_randFor_class = forest_reg_grid.predict(X_test_sample)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('rand_forrestReg',\n",
       "                 RandomForestRegressor(max_depth=2, min_samples_split=6,\n",
       "                                       n_estimators=1000))])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_reg_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjusted to best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg_grid.fit(X_train, y_train)\n",
    "y_pred_randFo = forest_reg_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('rand_forrestReg',\n",
       "                 RandomForestRegressor(max_depth=2, min_samples_split=6,\n",
       "                                       n_estimators=1000))])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_reg_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score = forest_reg_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.004472808329518907"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_Forrest_reg_pickle', 'wb') as forest_reg_file:\n",
    "    pickle.dump(forest_reg_grid, forest_reg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_Forrest_reg_pickle', 'rb') as forest_reg_file:\n",
    "    model_Forrest_reg_pickle = pickle.load(forest_reg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, SVR, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm_class = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'model_svm_class__kernel': ['linear'],\n",
    "                   'model_svm_class__C':[1],\n",
    "                   'model_svm_class__degree':[1]\n",
    "}\n",
    "SVC_grid_class = GridSearchCV(estimator=model_svm_class, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_class_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('model_svm_class', SVC())])\n",
    "smv_class_grid = GridSearchCV(estimator=svm_class_pipe, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with sample to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv_class_grid.fit(X_train_sample, y_trainCat_sample.ravel())\n",
    "y_pred_svm = smv_class_grid.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('model_svm_class', SVC(C=1, degree=1, kernel='linear'))])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smv_class_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjusted to best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv_class_grid.fit(X_train,y_train_cat)\n",
    "y_pred_svc_cat = smv_class_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('model_svm_class', SVC(C=1, degree=1, kernel='linear'))])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smv_class_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score = smv_class_grid.score(X_test, y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5673981191222572"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_smv_class_pickle', 'wb') as smv_class_file:\n",
    "    pickle.dump(smv_class_grid, smv_class_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_smv_class_pickle', 'rb') as smv_class_file:\n",
    "    model_smv_class_pickle = pickle.load(smv_class_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'xg_reg__objective' : ['reg:squarederror'],\n",
    "    'xg_reg__colsample_bytree':[0.1],\n",
    "    'xg_reg__n_estimators': [3000],\n",
    "    'xg_reg__max_depth': [4],\n",
    "    'xg_reg__learning_rate': [0.0001],\n",
    "    'xg_reg__alpha': [5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_class_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('xg_reg', xgb.XGBRegressor())])\n",
    "xgb_grid = GridSearchCV(estimator=svm_class_pipe, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with sample to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.fit(X_train_sample, y_train_sample.ravel())\n",
    "y_pred_xbg = xgb_grid.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('xg_reg',\n",
       "                 XGBRegressor(alpha=5, base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=0.1, gamma=0, gpu_id=-1,\n",
       "                              importance_type='gain',\n",
       "                              interaction_constraints='', learning_rate=0.0001,\n",
       "                              max_delta_step=0, max_depth=4, min_child_weight=1,\n",
       "                              missing=nan, monotone_constraints='()',\n",
       "                              n_estimators=3000, n_jobs=8, num_parallel_tree=1,\n",
       "                              random_state=0, reg_alpha=5, reg_lambda=1,\n",
       "                              scale_pos_weight=1, subsample=1,\n",
       "                              tree_method='exact', validate_parameters=1,\n",
       "                              verbosity=None))])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjusted to best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.fit(X_train,y_train)\n",
    "y_pred_xbg = xgb_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('xg_reg',\n",
       "                 XGBRegressor(alpha=30, base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=0.1, gamma=0, gpu_id=-1,\n",
       "                              importance_type='gain',\n",
       "                              interaction_constraints='', learning_rate=0.005,\n",
       "                              max_delta_step=0, max_depth=2, min_child_weight=1,\n",
       "                              missing=nan, monotone_constraints='()',\n",
       "                              n_estimators=500, n_jobs=8, num_parallel_tree=1,\n",
       "                              random_state=0, reg_alpha=30, reg_lambda=1,\n",
       "                              scale_pos_weight=1, subsample=1,\n",
       "                              tree_method='exact', validate_parameters=1,\n",
       "                              verbosity=None))])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011911026682224213"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score = xgb_grid.score(X_test, y_test)\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_xgb_grid_pickle', 'wb') as xgb_reg_file:\n",
    "    pickle.dump(xgb_grid, xgb_reg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_xgb_grid_pickle', 'rb') as xgb_reg_file:\n",
    "    model_xgb_grid_pickle = pickle.load(xgb_reg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have data from 2018 and 2019 to develop models. Use different evaluation metrics for each problem and compare the performance of different models.\n",
    "\n",
    "You are required to predict delays on **out of sample** data from **first 7 days (1st-7th) of January 2020** and to share the file with LighthouseLabs. Sample submission can be found in the file **_sample_submission.csv_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================\n",
    "## Stretch Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variables are **CARRIER_DELAY, WEATHER_DELAY, NAS_DELAY, SECURITY_DELAY, LATE_AIRCRAFT_DELAY**. We need to do additional transformations because these variables are not binary but continuos. For each flight that was delayed, we need to have one of these variables as 1 and others 0.\n",
    "\n",
    "It can happen that we have two types of delays with more than 0 minutes. In this case, take the bigger one as 1 and others as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is **CANCELLED**. The main problem here is going to be huge class imbalance. We have only very little cancelled flights with comparison to all flights. It is important to do the right sampling before training and to choose correct evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
