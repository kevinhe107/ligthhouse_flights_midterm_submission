{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "In this file, instructions how to approach the challenge can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work on different types of Machine Learning problems:\n",
    "\n",
    "- **Regression Problem**: The goal is to predict delay of flights.\n",
    "- **(Stretch) Multiclass Classification**: If the plane was delayed, we will predict what type of delay it is (will be).\n",
    "- **(Stretch) Binary Classification**: The goal is to predict if the flight will be cancelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Task: Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is **ARR_DELAY**. We need to be careful which columns to use and which don't. For example, DEP_DELAY is going to be the perfect predictor, but we can't use it because in real-life scenario, we want to predict the delay before the flight takes of --> We can use average delay from earlier days but not the one from the actual flight we predict.  \n",
    "\n",
    "For example, variables **CARRIER_DELAY, WEATHER_DELAY, NAS_DELAY, SECURITY_DELAY, LATE_AIRCRAFT_DELAY** shouldn't be used directly as predictors as well. However, we can create various transformations from earlier values.\n",
    "\n",
    "We will be evaluating your models by predicting the ARR_DELAY for all flights **1 week in advance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering will play a crucial role in this problems. We have only very little attributes so we need to create some features that will have some predictive power.\n",
    "\n",
    "- weather: we can use some weather API to look for the weather in time of the scheduled departure and scheduled arrival.\n",
    "- statistics (avg, mean, median, std, min, max...): we can take a look at previous delays and compute descriptive statistics\n",
    "- airports encoding: we need to think about what to do with the airports and other categorical variables\n",
    "- time of the day: the delay probably depends on the airport traffic which varies during the day.\n",
    "- airport traffic\n",
    "- unsupervised learning as feature engineering?\n",
    "- **what are the additional options?**: Think about what we could do more to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = pd.read_csv('flights_big.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dropping massive null columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flights.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1 = flights.drop(['carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay',\n",
    "                          'first_dep_time', 'total_add_gtime', 'longest_add_gtime','cancellation_code', \n",
    "                          'no_name', 'dup', ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fl_date', 'mkt_unique_carrier', 'branded_code_share', 'mkt_carrier',\n",
       "       'mkt_carrier_fl_num', 'op_unique_carrier', 'tail_num',\n",
       "       'op_carrier_fl_num', 'origin_airport_id', 'origin', 'origin_city_name',\n",
       "       'dest_airport_id', 'dest', 'dest_city_name', 'crs_dep_time', 'dep_time',\n",
       "       'dep_delay', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in',\n",
       "       'crs_arr_time', 'arr_time', 'arr_delay', 'cancelled', 'diverted',\n",
       "       'crs_elapsed_time', 'actual_elapsed_time', 'air_time', 'flights',\n",
       "       'distance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dropping columns that mean the same thing and certain columns that don't mean anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1 = flights1.drop(['mkt_carrier', 'op_unique_carrier', 'flights', 'mkt_carrier_fl_num', \n",
    "                          'tail_num', 'branded_code_share', 'op_carrier_fl_num', 'cancelled', 'diverted',\n",
    "                         'wheels_on', 'wheels_off'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flights1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filling Nan with modes for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_time_mode = flights1['dep_time'].mode() # one mode\n",
    "arr_time_mode = flights1['arr_time'].mode() # one mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1['dep_time'] = flights1['dep_time'].fillna(int(dep_time_mode))\n",
    "flights1['arr_time'] = flights1['arr_time'].fillna(int(arr_time_mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filling nan with means for continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1['arr_delay'] = flights1['arr_delay'].fillna(flights1['arr_delay'].mean())\n",
    "flights1['dep_delay'] = flights1['dep_delay'].fillna(flights1['dep_delay'].mean())\n",
    "flights1['taxi_out'] = flights1['taxi_out'].fillna(flights1['taxi_out'].mean())\n",
    "flights1['taxi_in'] = flights1['taxi_in'].fillna(flights1['taxi_in'].mean())\n",
    "flights1['crs_elapsed_time'] = flights1['crs_elapsed_time'].fillna(flights1['crs_elapsed_time'].mean())\n",
    "flights1['actual_elapsed_time'] = flights1['actual_elapsed_time'].fillna(flights1['actual_elapsed_time'].mean())\n",
    "flights1['air_time'] = flights1['air_time'].fillna(flights1['air_time'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adding month, day of the week, day of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1['fl_date'] = pd.to_datetime(flights1['fl_date'], errors='coerce')\n",
    "flights1['month'] = flights1['fl_date'].dt.month\n",
    "flights1['day_of_week'] = flights1['fl_date'].dt.dayofweek\n",
    "flights1['day_of_month'] = flights1['fl_date'].dt.day\n",
    "flights1['year'] = flights1['fl_date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* splitting origin_city_name and dest_city_name from its short version name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1[['origin_city_name_only', 'origin_city_name_short']] = flights1['origin_city_name'].str.split(',', expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1[['dest_city_name_only', 'dest_city_name_short']] = flights1['dest_city_name'].str.split(',', expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping short hand version of city names\n",
    "flights1 = flights1.drop(['origin_city_name_short', 'dest_city_name_short', 'origin_city_name', 'dest_city_name', 'dest', 'origin'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1['origin_city_name_only'] = flights1['origin_city_name_only'].str.strip().str.lower()\n",
    "flights1['dest_city_name_only'] = flights1['dest_city_name_only'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* make hour departure and arrival feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1['hour_departure'] = flights1['dep_time'].apply(\n",
    "    lambda x: str(x)[:2] if len(str(x)) == 6 else (str(x)[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1['hour_arrival'] = flights1['arr_time'].apply(\n",
    "    lambda x: str(x)[:2] if len(str(x)) == 6 else (str(x)[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping original departure and arrival features\n",
    "flights1 = flights1.drop(['dep_time', 'arr_time'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorize long, medium and short flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Short     434745\n",
       "Medium     63415\n",
       "Long        1840\n",
       "Name: flight_duration_type, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting to air_time\n",
    "def flight_duration(x):\n",
    "    if x <=180:\n",
    "        return 'Short'\n",
    "    elif x >180 and x<360:\n",
    "        return 'Medium'\n",
    "    elif x>=360:\n",
    "        return 'Long'\n",
    "\n",
    "flights1['flight_duration_type']=flights1['air_time'].apply(lambda x: flight_duration(x))\n",
    "flights1['flight_duration_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping original air_time feature\n",
    "flights1 = flights1.drop(['air_time', 'actual_elapsed_time'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* taxi-in categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "short_taxi_in     464354\n",
       "medium_taxi_in     34623\n",
       "long_taxi_in        1023\n",
       "Name: taxi_in_duration, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def taxi_in_duration(x):\n",
    "    if x <=15:\n",
    "        return 'short_taxi_in'\n",
    "    elif x > 15 and x<50:\n",
    "        return 'medium_taxi_in'\n",
    "    else:\n",
    "        return 'long_taxi_in'\n",
    "\n",
    "flights1['taxi_in_duration']=flights1['taxi_in'].apply(lambda x: taxi_in_duration(x))\n",
    "flights1['taxi_in_duration'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* taxi-out categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "short_taxi_out     432318\n",
       "medium_taxi_out     65567\n",
       "long_taxi_out        2115\n",
       "Name: taxi_out_duration, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def taxi_out_duration(x):\n",
    "    if x <=25:\n",
    "        return 'short_taxi_out'\n",
    "    elif x > 25 and x<70:\n",
    "        return 'medium_taxi_out'\n",
    "    else:\n",
    "        return 'long_taxi_out'\n",
    "\n",
    "flights1['taxi_out_duration']=flights1['taxi_out'].apply(lambda x: taxi_out_duration(x))\n",
    "flights1['taxi_out_duration'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dropping taxi_in and taxi_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1 = flights1.drop(['taxi_in', 'taxi_out'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* arr_delay per unique_carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_arrdelay_carrier = flights1.groupby('mkt_unique_carrier')['arr_delay'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_arrdelay_carrier.name = 'mean_arrdelay_carrier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1 = pd.merge(flights1, mean_arrdelay_carrier, how = 'left', on = ['mkt_unique_carrier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* arrival delay per dest_airport id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_arrdelay_dest_air = flights1.groupby('dest_airport_id')['arr_delay'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_arrdelay_dest_air.name = 'mean_arrdelay_dest_air'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1 = pd.merge(flights1, mean_arrdelay_dest_air, how = 'left', on = ['dest_airport_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* arrival delay per origin_airport id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_arrdelay_origin_air = flights1.groupby('origin_airport_id')['arr_delay'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_arrdelay_origin_air.name = 'mean_arrdelay_origin_air'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1 = pd.merge(flights1, mean_arrdelay_origin_air, how = 'left', on = ['origin_airport_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make the types categories from feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights1[\"mkt_unique_carrier\"] = flights1[\"mkt_unique_carrier\"].astype(\"category\")\n",
    "flights1[\"origin_airport_id\"] = flights1[\"origin_airport_id\"].astype(\"category\")\n",
    "flights1[\"dest_airport_id\"] = flights1[\"dest_airport_id\"].astype(\"category\")\n",
    "flights1[\"flight_duration_type\"] = flights1[\"flight_duration_type\"].astype(\"category\")\n",
    "flights1[\"taxi_in_duration\"] = flights1[\"taxi_in_duration\"].astype(\"category\")\n",
    "flights1[\"taxi_out_duration\"] = flights1[\"taxi_out_duration\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode arrival delay\n",
    "flights1['arr_delay_cat'] = flights1['arr_delay'].apply(lambda x: 1 if x > 0 else 0)\n",
    "# ENCODE AIRPORTS AND TAILNUM\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "flights1['mkt_unique_carrier'] = encoder.fit_transform(flights1[['mkt_unique_carrier']])\n",
    "flights1['origin_airport_id'] = encoder.fit_transform(flights1[['origin_airport_id']])\n",
    "flights1['dest_airport_id'] = encoder.fit_transform(flights1[['dest_airport_id']])\n",
    "flights1['taxi_in_duration'] = encoder.fit_transform(flights1[['taxi_in_duration']])\n",
    "flights1['taxi_out_duration'] = encoder.fit_transform(flights1[['taxi_out_duration']])\n",
    "flights1['flight_duration_type'] = encoder.fit_transform(flights1[['flight_duration_type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* join on weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['City'] = weather['City'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['date'] = pd.to_datetime(weather['StartTime(UTC)']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['EndTime(UTC)'] = pd.to_datetime(weather['EndTime(UTC)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = weather.groupby(['City', 'date']).max()['EndTime(UTC)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_merged = weather.merge(temp, on = ['City', 'date', 'EndTime(UTC)'], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_merged = weather_merged.rename(columns = {'date': 'fl_date', 'City': 'origin_city_name_only'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_merged = weather_merged.drop(['StartTime(UTC)', 'EndTime(UTC)'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_merged['fl_date'] = pd.to_datetime(weather_merged['fl_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather = flights1.merge(weather_merged, on = ['origin_city_name_only', 'fl_date'], how = 'left') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dropping flights_weather row null values, origin_city_name_only, dest_city_name_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather = flights_weather.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather = flights_weather.drop(['fl_date', 'origin_city_name_only', 'dest_city_name_only'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Categorize and encode weather and severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather[\"Type\"] = flights_weather[\"Type\"].astype(\"category\")\n",
    "flights_weather[\"Severity\"] = flights_weather[\"Severity\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather['Type'] = encoder.fit_transform(flights_weather[['Type']])\n",
    "flights_weather['Severity'] = encoder.fit_transform(flights_weather[['Severity']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dropping dep_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_weather = flights_weather.drop('dep_delay', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133948, 23)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mkt_unique_carrier', 'origin_airport_id', 'dest_airport_id',\n",
       "       'crs_dep_time', 'crs_arr_time', 'arr_delay', 'crs_elapsed_time',\n",
       "       'distance', 'month', 'day_of_week', 'day_of_month', 'year',\n",
       "       'hour_departure', 'hour_arrival', 'flight_duration_type',\n",
       "       'taxi_in_duration', 'taxi_out_duration', 'mean_arrdelay_carrier',\n",
       "       'mean_arrdelay_dest_air', 'mean_arrdelay_origin_air', 'arr_delay_cat',\n",
       "       'Type', 'Severity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_weather.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection / Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply different selection techniques to find out which one will be the best for our problems.\n",
    "\n",
    "- Original Features vs. PCA conponents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PCA testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.decomposition import PCA\\nimport matplotlib.pyplot as plt'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different ML techniques to predict each problem.\n",
    "\n",
    "- linear\n",
    "- Naive Bayes\n",
    "- Random Forest Regressor\n",
    "- SVM classification\n",
    "- XGBoost regresspr\n",
    "- The ensemble of your own choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickle module to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* target and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(flights_weather.arr_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = np.array(flights_weather.arr_delay_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = flights_weather.drop(['arr_delay', 'arr_delay_cat'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133948, 21)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mkt_unique_carrier', 'origin_airport_id', 'dest_airport_id',\n",
       "       'crs_dep_time', 'crs_arr_time', 'crs_elapsed_time', 'distance', 'month',\n",
       "       'day_of_week', 'day_of_month', 'year', 'hour_departure', 'hour_arrival',\n",
       "       'flight_duration_type', 'taxi_in_duration', 'taxi_out_duration',\n",
       "       'mean_arrdelay_carrier', 'mean_arrdelay_dest_air',\n",
       "       'mean_arrdelay_origin_air', 'Type', 'Severity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train test split and making samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.70,test_size=0.30, random_state=101, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating y_train_cat and y_test_cat\n",
    "X_train, X_test, y_train_cat, y_test_cat = model_selection.train_test_split(X, y_cat, train_size=0.70,test_size=0.30, random_state=101, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = pd.DataFrame(X_train).sample(frac = 0.1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sample = pd.DataFrame(y_train).sample(frac = 0.1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainCat_sample = pd.DataFrame(y_train_cat).sample(frac = 0.1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sample = pd.DataFrame(X_test).sample(frac = 0.1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scaler = StandardScaler()\\nX = scaler.fit_transform(X)'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y = (y - y.mean()) / y.std()'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''y = (y - y.mean()) / y.std()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scaling PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler())])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler())])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('linear_model', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_model', LinearRegression())])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_linear = linear_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score = linear_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08445854757542315"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_linear_pickle', 'wb') as linear_file:\n",
    "    pickle.dump(linear_pipe, linear_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_linear_pickle', 'rb') as linear_file:\n",
    "    model_linear = pickle.load(linear_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear_ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elasnet_model = ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters ={\n",
    "    'elasnet_model__alpha': [1],\n",
    "    'elasnet_model__l1_ratio': [0.0001]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasnet_model_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('elasnet_model', ElasticNet())])\n",
    "elasnet_model_grid = GridSearchCV(estimator=elasnet_model_pipe, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with sample to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elasnet_model_grid.fit(X_train_sample, y_trainCat_sample.ravel())\\ny_pred_randFor_class = elasnet_model_grid.predict(X_test_sample)'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''elasnet_model_grid.fit(X_train_sample, y_trainCat_sample.ravel())\n",
    "y_pred_randFor_class = elasnet_model_grid.predict(X_test_sample)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elasnet_model_grid.best_estimator_'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''elasnet_model_grid.best_estimator_'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjusted to best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('elasnet_model', ElasticNet())]),\n",
       "             param_grid={'elasnet_model__alpha': [1],\n",
       "                         'elasnet_model__l1_ratio': [0.0001]},\n",
       "             scoring='r2')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasnet_model_grid.fit(X_train, y_train_cat.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_randFor_class = elasnet_model_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('elasnet_model', ElasticNet(alpha=1, l1_ratio=0.0001))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasnet_model_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09565149757045965"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score = elasnet_model_grid.score(X_test, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_linear_Ela_pickle', 'wb') as linear_Elas_file:\n",
    "    pickle.dump(elasnet_model_grid, linear_Elas_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_linear_Ela_pickle', 'rb') as linear_Elas_file:\n",
    "    model_elastic_linear = pickle.load(linear_Elas_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polyfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model = PolynomialFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('poly_model', PolynomialFeatures(2)), ('linear_reg', LinearRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with sample to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model_pipe.fit(X_train_sample, y_train_sample.ravel())\n",
    "y_pred_poly = poly_model_pipe.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('poly_model', PolynomialFeatures()),\n",
       "                ('linear_reg', LinearRegression())])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_model_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-514.8836521674519"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score = poly_model_pipe.score(X_test, y_test_cat)\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naives Bayes, GaussianNB Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_Gauss_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_Gauss_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('NB_Gauss_model', GaussianNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('NB_Gauss_model', GaussianNB())])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_Gauss_pipe.fit(X_train, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_NBGauss = NB_Gauss_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6662933930571109"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score = NB_Gauss_pipe.score(X_test, y_test_cat)\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_Gaussian_pickle', 'wb') as Gaussian_file:\n",
    "    pickle.dump(NB_Gauss_pipe, Gaussian_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_Gaussian_pickle', 'rb') as Gaussian_file:\n",
    "    model_Gaussian = pickle.load(Gaussian_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_class = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters ={\n",
    "    'rand_forrestClass__n_estimators': [8000, 10000],\n",
    "    'rand_forrestClass__max_depth': [10],\n",
    "    'rand_forrestClass__bootstrap': [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forest_class_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('rand_forrestClass', RandomForestClassifier())])\n",
    "forest_class_grid = GridSearchCV(estimator=Forest_class_pipe, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with sample to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forrestClass_grid = GridSearchCV(estimator=random_forest_class, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_class_grid.fit(X_train_sample, y_trainCat_sample.ravel())\n",
    "y_pred_randFor_class = forest_class_grid.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_class_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjusted to best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_class_grid.fit(X_train, y_train_cat.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_randFor_class = forest_class_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_class_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5616250137111254"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score = forest_class_grid.score(X_test, y_test_cat)\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_Forrest_class_pickle', 'wb') as forest_class_file:\n",
    "    pickle.dump(forest_class_grid, forest_class_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_Forrest_class_pickle', 'rb') as forest_class_file:\n",
    "    model_forrestClass = pickle.load(forest_class_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regression_model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'rand_forrestReg__n_estimators': [1000],\n",
    "    'rand_forrestReg__max_depth': [2],\n",
    "    'rand_forrestReg__min_samples_split':[6], \n",
    "    'rand_forrestReg__bootstrap':[True],\n",
    "    'rand_forrestReg__criterion' :['mse']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forest_reg_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('rand_forrestReg', RandomForestRegressor())])\n",
    "forest_reg_grid = GridSearchCV(estimator=Forest_reg_pipe, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with sample to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''forest_reg_grid.fit(X_train_sample, y_train_sample.ravel())\n",
    "y_pred_randFor_class = forest_reg_grid.predict(X_test_sample)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('rand_forrestReg',\n",
       "                 RandomForestRegressor(max_depth=2, min_samples_split=6,\n",
       "                                       n_estimators=1000))])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_reg_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjusted to best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg_grid.fit(X_train, y_train)\n",
    "y_pred_randFo = forest_reg_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('rand_forrestReg',\n",
       "                 RandomForestRegressor(max_depth=2, min_samples_split=6,\n",
       "                                       n_estimators=1000))])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_reg_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score = forest_reg_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.004472808329518907"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_Forrest_reg_pickle', 'wb') as forest_reg_file:\n",
    "    pickle.dump(forest_reg_grid, forest_reg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_Forrest_reg_pickle', 'rb') as forest_reg_file:\n",
    "    model_Forrest_reg_pickle = pickle.load(forest_reg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, SVR, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm_class = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'model_svm_class__kernel': ['linear'],\n",
    "                   'model_svm_class__C':[1],\n",
    "                   'model_svm_class__degree':[1]\n",
    "}\n",
    "SVC_grid_class = GridSearchCV(estimator=model_svm_class, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_class_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('model_svm_class', SVC())])\n",
    "smv_class_grid = GridSearchCV(estimator=svm_class_pipe, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with sample to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv_class_grid.fit(X_train_sample, y_trainCat_sample.ravel())\n",
    "y_pred_svm = smv_class_grid.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('model_svm_class', SVC(C=1, degree=1, kernel='linear'))])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smv_class_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjusted to best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv_class_grid.fit(X_train,y_train_cat)\n",
    "y_pred_svc_cat = smv_class_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('model_svm_class', SVC(C=1, degree=1, kernel='linear'))])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smv_class_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score = smv_class_grid.score(X_test, y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5673981191222572"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_smv_class_pickle', 'wb') as smv_class_file:\n",
    "    pickle.dump(smv_class_grid, smv_class_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_smv_class_pickle', 'rb') as smv_class_file:\n",
    "    model_smv_class_pickle = pickle.load(smv_class_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'xg_reg__objective' : ['reg:squarederror'],\n",
    "    'xg_reg__colsample_bytree':[0.1],\n",
    "    'xg_reg__n_estimators': [3000],\n",
    "    'xg_reg__max_depth': [4],\n",
    "    'xg_reg__learning_rate': [0.0001],\n",
    "    'xg_reg__alpha': [5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_class_pipe = Pipeline(steps=[('scaler', StandardScaler()), ('xg_reg', xgb.XGBRegressor())])\n",
    "xgb_grid = GridSearchCV(estimator=svm_class_pipe, param_grid=hyperparameters, scoring = 'r2', verbose=0, cv= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with sample to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.fit(X_train_sample, y_train_sample.ravel())\n",
    "y_pred_xbg = xgb_grid.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('xg_reg',\n",
       "                 XGBRegressor(alpha=5, base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=0.1, gamma=0, gpu_id=-1,\n",
       "                              importance_type='gain',\n",
       "                              interaction_constraints='', learning_rate=0.0001,\n",
       "                              max_delta_step=0, max_depth=4, min_child_weight=1,\n",
       "                              missing=nan, monotone_constraints='()',\n",
       "                              n_estimators=3000, n_jobs=8, num_parallel_tree=1,\n",
       "                              random_state=0, reg_alpha=5, reg_lambda=1,\n",
       "                              scale_pos_weight=1, subsample=1,\n",
       "                              tree_method='exact', validate_parameters=1,\n",
       "                              verbosity=None))])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjusted to best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.fit(X_train,y_train)\n",
    "y_pred_xbg = xgb_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('xg_reg',\n",
       "                 XGBRegressor(alpha=30, base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=0.1, gamma=0, gpu_id=-1,\n",
       "                              importance_type='gain',\n",
       "                              interaction_constraints='', learning_rate=0.005,\n",
       "                              max_delta_step=0, max_depth=2, min_child_weight=1,\n",
       "                              missing=nan, monotone_constraints='()',\n",
       "                              n_estimators=500, n_jobs=8, num_parallel_tree=1,\n",
       "                              random_state=0, reg_alpha=30, reg_lambda=1,\n",
       "                              scale_pos_weight=1, subsample=1,\n",
       "                              tree_method='exact', validate_parameters=1,\n",
       "                              verbosity=None))])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011911026682224213"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score = xgb_grid.score(X_test, y_test)\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_xgb_grid_pickle', 'wb') as xgb_reg_file:\n",
    "    pickle.dump(xgb_grid, xgb_reg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_xgb_grid_pickle', 'rb') as xgb_reg_file:\n",
    "    model_xgb_grid_pickle = pickle.load(xgb_reg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have data from 2018 and 2019 to develop models. Use different evaluation metrics for each problem and compare the performance of different models.\n",
    "\n",
    "You are required to predict delays on **out of sample** data from **first 7 days (1st-7th) of January 2020** and to share the file with LighthouseLabs. Sample submission can be found in the file **_sample_submission.csv_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================\n",
    "## Stretch Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variables are **CARRIER_DELAY, WEATHER_DELAY, NAS_DELAY, SECURITY_DELAY, LATE_AIRCRAFT_DELAY**. We need to do additional transformations because these variables are not binary but continuos. For each flight that was delayed, we need to have one of these variables as 1 and others 0.\n",
    "\n",
    "It can happen that we have two types of delays with more than 0 minutes. In this case, take the bigger one as 1 and others as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is **CANCELLED**. The main problem here is going to be huge class imbalance. We have only very little cancelled flights with comparison to all flights. It is important to do the right sampling before training and to choose correct evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
